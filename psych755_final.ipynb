{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8AiyJoBgjr6xGOd5P0DA4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmodahl/PSYCH-755-Final/blob/main/psych755_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In the following notebook, I will be using the IMDB sentiment analysis dataset from keras to create a neural network, which is able to accurately classify positive vs. negative reviews. The dataset includes around 50,000 reviews from IMDB, which are labeled as positive or negative."
      ],
      "metadata": {
        "id": "vgHK6kq9s639"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "oKcO0wUlt792"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0S2SqXVDswLu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # necessary for importing keras\n",
        "from tensorflow import keras # keras includes the dataset and other neural network tools we're using\n",
        "from keras.datasets import imdb # imdb dataset\n",
        "import numpy as np # for defining arrays\n",
        "from keras.models import Sequential # for modeling\n",
        "from keras.wrappers.scikit_learn import KerasClassifier # for creating the model\n",
        "from keras import layers # for defining layers in model\n",
        "from keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense # for model parameters\n",
        "from sklearn.model_selection import ParameterGrid # for creating the model parameter grid\n",
        "from keras.callbacks import EarlyStopping # used in parameter grid\n",
        "from sklearn.metrics import accuracy_score # for calculating test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting data into train and test sets"
      ],
      "metadata": {
        "id": "nc96n7hIuFHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
      ],
      "metadata": {
        "id": "CzT-72_-uAfZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoding the reviews\n",
        "This dataset is a bit confusing since the text of the reviews are actually integer-encoded. This means that the value of each review is just a sequence of integers with each integer representing a specific word in the dictionary. Because of this, we need to convert the integers into more meaningful text values which we will be doing below."
      ],
      "metadata": {
        "id": "Pe87we_su3ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example review"
      ],
      "metadata": {
        "id": "alP9zerMvcab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJOmLlAdvgnL",
        "outputId": "6530e1ae-7231-4960-dd38-0352adaba020"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding from integer to text"
      ],
      "metadata": {
        "id": "Tg--KXzgvwP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "index_from = 3\n",
        "word_index = {key:(value+index_from) for key,value in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "\n",
        "reverse_word_index = {value:key for key, value in word_index.items()}\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "# source: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/get_word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "P6A5zPRku0mQ",
        "outputId": "2a91386c-f06e-4001-d31e-53897c2eb8e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Same review as above, but now decoded"
      ],
      "metadata": {
        "id": "nBRerVXqxscf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decode_review(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "vcntfqejxwhb",
        "outputId": "51981905-aa06-469d-9e40-2e008a26fdf3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining train, validation, and test sequences"
      ],
      "metadata": {
        "id": "pnvh1P_uyvhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = 2500 # Only using the 2500 most common words (you can use any amount, I just decided to use 2500)\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words= total_words)\n",
        "\n",
        "X_train, X_val = X_train[:-2500], X_train[-2500:]\n",
        "y_train, y_val = y_train[:-2500], y_train[-2500:]\n",
        "\n",
        "print(len(X_train)) # number of training sequences\n",
        "print(len(X_val)) # number of validation sequences\n",
        "print(len(X_test)) # number of test sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plZ-H8pIx6DA",
        "outputId": "55a70188-2f8e-4408-bafa-e0a7c166c5ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22500\n",
            "2500\n",
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Same review as before, but now with less common words removed"
      ],
      "metadata": {
        "id": "kLXh8STWz6aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decode_review(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "ERlDYhFVzR6q",
        "outputId": "5cf4775c-30ac-40ab-93ec-7a868a6dd01d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction <UNK> really <UNK> the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same <UNK> island as myself so i loved the fact there was a real connection with this film the witty <UNK> throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really <UNK> at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of <UNK> and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was <UNK> life after all that was <UNK> with us all\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardize length of reviews:\n",
        "Here, we will be using the pad_sequences function from keras to standardize the length of the reviews in this dataset. We do this because the length of the reviews can vary greatly and this can effect the accuracy of our sentiment analysis."
      ],
      "metadata": {
        "id": "-5wniwcp1QTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = 500 # setting a 500 word limit for all reviews\n",
        "\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train, value= word_index[\"<PAD>\"], padding= 'post', maxlen= max_sequence_length)\n",
        "X_val = keras.preprocessing.sequence.pad_sequences(X_val, value= word_index[\"<PAD>\"], padding= 'post', maxlen= max_sequence_length)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, value= word_index[\"<PAD>\"], padding= 'post', maxlen= max_sequence_length)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "# source: https://note.com/mlai/n/ndd0643e2a843"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNw-yjP30tMd",
        "outputId": "34d3b076-e284-4aad-e05e-8df9358f2ce3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(22500, 500)\n",
            "(2500, 500)\n",
            "(25000, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "fM6LbtaJ5PxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "\n",
        "def create_model(filters = 64, kernel_size = 3, strides=1, units = 256,\n",
        "                 optimizer='adam', rate = 0.25, kernel_initializer ='glorot_uniform'): # model parameters\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Embedding(total_words, embedding_dim, input_length= max_sequence_length)) # embedding layer\n",
        "\n",
        "    model.add(Dropout(rate))\n",
        "    model.add(Conv1D(filters = filters, kernel_size = kernel_size, strides= strides,\n",
        "                     padding='same', activation= 'relu')) # convolutional layers\n",
        "\n",
        "    model.add(GlobalMaxPooling1D()) # max pooling\n",
        "\n",
        "    model.add(Dense(units = units, activation= 'relu', kernel_initializer= kernel_initializer)) # dense layer\n",
        "    model.add(Dropout(rate)) # dropout\n",
        "\n",
        "    model.add(Dense(1, activation= 'sigmoid')) # output layer\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer= optimizer,\n",
        "                  metrics=['accuracy']) # compile the model\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn= create_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwmKHhKh5PLr",
        "outputId": "1ff25d5f-9e25-4720-ec60-c74e9df8e957"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-7665640bcc8f>:25: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn= create_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning hyperparameters"
      ],
      "metadata": {
        "id": "4SDJ4Msn6hq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "filters = [128]\n",
        "kernel_size = [5]\n",
        "strides = [1]\n",
        "Dense_units = [128, 512]\n",
        "kernel_initializer = ['TruncatedNormal']\n",
        "rate_dropouts = [0.25]\n",
        "optimizers = ['adam']\n",
        "epochs = [5]\n",
        "batches = [64]\n",
        "\n",
        "# parameter grid search\n",
        "param_grid = dict(optimizer = optimizers, epochs = epochs, batch_size = batches,\n",
        "                  filters = filters, kernel_size = kernel_size, strides = strides,\n",
        "                  units = Dense_units, kernel_initializer = kernel_initializer, rate = rate_dropouts)\n",
        "\n",
        "grid = ParameterGrid(param_grid)\n",
        "param_sets = list(grid)\n",
        "\n",
        "param_scores = []\n",
        "for params in grid:\n",
        "\n",
        "    print(params)\n",
        "    model.set_params(**params)\n",
        "\n",
        "    earlystopper = EarlyStopping(monitor='val_accuracy', patience= 0, verbose=1)\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        shuffle= True,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        callbacks= [earlystopper])\n",
        "\n",
        "    param_score = history.history['val_accuracy']\n",
        "    param_scores.append(param_score[-1])\n",
        "    print('+-'*50)\n",
        "\n",
        "print('param_scores:', param_scores)\n",
        "\n",
        "# Choose best parameters\n",
        "p = np.argmax(np.array(param_scores))\n",
        "best_params = param_sets[p]\n",
        "print(\"best parameter set\", best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge-q7KGA6J9k",
        "outputId": "e0d9d879-4466-4c1b-a787-809ce119d828"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_initializer': 'TruncatedNormal', 'kernel_size': 5, 'optimizer': 'adam', 'rate': 0.25, 'strides': 1, 'units': 128}\n",
            "Epoch 1/5\n",
            "352/352 [==============================] - 42s 115ms/step - loss: 0.5445 - accuracy: 0.6900 - val_loss: 0.3451 - val_accuracy: 0.8576\n",
            "Epoch 2/5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3170 - accuracy: 0.8672 - val_loss: 0.3025 - val_accuracy: 0.8728\n",
            "Epoch 3/5\n",
            "352/352 [==============================] - 36s 103ms/step - loss: 0.2671 - accuracy: 0.8910 - val_loss: 0.2856 - val_accuracy: 0.8824\n",
            "Epoch 4/5\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2333 - accuracy: 0.9070 - val_loss: 0.2869 - val_accuracy: 0.8836\n",
            "Epoch 5/5\n",
            "352/352 [==============================] - 37s 107ms/step - loss: 0.2107 - accuracy: 0.9186 - val_loss: 0.2803 - val_accuracy: 0.8832\n",
            "Epoch 5: early stopping\n",
            "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
            "{'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_initializer': 'TruncatedNormal', 'kernel_size': 5, 'optimizer': 'adam', 'rate': 0.25, 'strides': 1, 'units': 512}\n",
            "Epoch 1/5\n",
            "352/352 [==============================] - 38s 106ms/step - loss: 0.5158 - accuracy: 0.7132 - val_loss: 0.3524 - val_accuracy: 0.8528\n",
            "Epoch 2/5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3170 - accuracy: 0.8642 - val_loss: 0.3040 - val_accuracy: 0.8768\n",
            "Epoch 3/5\n",
            "352/352 [==============================] - 39s 110ms/step - loss: 0.2617 - accuracy: 0.8911 - val_loss: 0.2722 - val_accuracy: 0.8936\n",
            "Epoch 4/5\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2297 - accuracy: 0.9059 - val_loss: 0.3049 - val_accuracy: 0.8796\n",
            "Epoch 4: early stopping\n",
            "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
            "param_scores: [0.8831999897956848, 0.8795999884605408]\n",
            "best parameter set {'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_initializer': 'TruncatedNormal', 'kernel_size': 5, 'optimizer': 'adam', 'rate': 0.25, 'strides': 1, 'units': 128}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running model with best parameters"
      ],
      "metadata": {
        "id": "WFX6FV-hZrPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.set_params(**best_params)\n",
        "model.fit(np.vstack((X_train, X_val)), np.hstack((y_train, y_val)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sRq7FoL_fjs",
        "outputId": "8630897b-d7df-4806-b004-978055b05e37"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 40s 99ms/step - loss: 0.5376 - accuracy: 0.7028\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 40s 103ms/step - loss: 0.3114 - accuracy: 0.8688\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.2583 - accuracy: 0.8967\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2313 - accuracy: 0.9099\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 41s 104ms/step - loss: 0.2084 - accuracy: 0.9195\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7abf7d9bba00>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test accuracy of model"
      ],
      "metadata": {
        "id": "TnWmrYdaZuZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test accuracy = %f%%\" % (accuracy_score(y_test, model.predict(X_test))*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3F4qET8_h29",
        "outputId": "886519f2-a2ea-4c3d-83e3-c3e44f4cda03"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 17s 21ms/step\n",
            "Test accuracy = 81.876000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "Although the code I worked on for this project does not directly relate to the Healthy Minds project I've been working on, I could see potential for the use of a neural network model in the future with the Healthy Minds data. For example, perhaps Healthy Minds could use a neural network model to predict whether or not participants have high or low ACIP (Awareness, Connection, Insight, Purpose) scores from the responses they gave on certain survey items. Alternatively, Healthy Minds could use a similar kind of sentiment analysis to quickly calculate the total negative vs. positive reviews for their app. Additionally, I just found it to be some great neural network practice since I'm sure these kinds of models will pop up in my work in the near future."
      ],
      "metadata": {
        "id": "LEVmxQfAadV9"
      }
    }
  ]
}